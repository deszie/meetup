{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing for label 1\n",
      "Done writing for label 0\n",
      "Done spliting train/pos/\n",
      "Done spliting train/neg/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "#download the text files at https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "#and place them in the \"data/\" folder\n",
    "\n",
    "amazon = pd.read_csv(\"data/amazon_cells_labelled.txt\", sep=\"\\t\", header=None).rename(columns={0: 'text', 1: \"label\"})\n",
    "imdb = pd.read_csv(\"data/imdb_labelled.txt\", sep=\"\\t\", header=None).rename(columns={0: 'text', 1: \"label\"})\n",
    "yelp = pd.read_csv(\"data/yelp_labelled.txt\", sep=\"\\t\", header=None).rename(columns={0: 'text', 1: \"label\"})\n",
    "\n",
    "\n",
    "# write each sentence in text file for a specific folder\n",
    "def write_txt(string,path):\n",
    "    with open(\"%s.txt\" %(path), \"w\") as text_file:\n",
    "        text_file.write(string)\n",
    "    return\n",
    "\n",
    "def build_corpus(df,label):\n",
    "    if label==1:\n",
    "        folder = \"train/pos/\"\n",
    "    else:\n",
    "        folder = \"train/neg/\"\n",
    "        \n",
    "    df = list(df[df[\"label\"]==label][\"text\"])\n",
    "    for i in range(0,len(df)):\n",
    "        write_txt(df[i],folder+str(i))\n",
    "    print(\"Done writing for label %s\" %(label))\n",
    "    return\n",
    "        \n",
    "\n",
    "def test_split(folder,proportion):\n",
    "    text_files = [f for f in os.listdir(folder) if f.endswith('.txt')]\n",
    "    size = int(proportion*len(text_files))\n",
    "    sample = np.random.choice(range(0,len(text_files)),size=size)\n",
    "    sample = list(set(sample))\n",
    "    move = [text_files[i] for i in sample]\n",
    "    #print(move)\n",
    "    for element in move:\n",
    "        #print(element)\n",
    "        #os.rename(folder+element, folder+\"dev/\"+element)\n",
    "        shutil.move(folder+element, folder.replace(\"train\",\"test\"))\n",
    "    print(\"Done spliting %s\" %(folder))\n",
    "    return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#to run once\n",
    "\n",
    "#if folder does not exist, create it\n",
    "folder_list= [\"train/pos/\",\"train/neg/\",\"test/pos/\",\"test/neg/\"]\n",
    "for directory in folder_list:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "build_corpus(amazon,1)\n",
    "build_corpus(amazon,0)\n",
    "test_split(\"train/pos/\",0.1)\n",
    "test_split(\"train/neg/\",0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import plac\n",
    "import collections\n",
    "import random\n",
    "import pathlib\n",
    "import cytoolz\n",
    "import numpy\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "import pickle\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: model: File exists\r\n"
     ]
    }
   ],
   "source": [
    "%mkdir model\n",
    "import os, sys\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dir = current_dir+\"/model\"\n",
    "train_dir = current_dir+\"/train\"\n",
    "dev_dir = current_dir+\"/dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [-r] [-H 64] [-L 100] [-d 0.5] [-e 0.001] [-i 5]\n",
      "                   [-b 100] [-n -1]\n",
      "                   model_dir train_dir dev_dir\n",
      "__main__.py: error: the following arguments are required: train_dir, dev_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annjieching/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class SentimentAnalyser(object):\n",
    "    @classmethod\n",
    "    def load(cls, path, nlp, max_length=100):\n",
    "        with (path / 'config.json').open() as file_:\n",
    "            model = model_from_json(file_.read())\n",
    "        with (path / 'model').open('rb') as file_:\n",
    "            lstm_weights = pickle.load(file_)\n",
    "        embeddings = get_embeddings(nlp.vocab)\n",
    "        model.set_weights([embeddings] + lstm_weights)\n",
    "        return cls(model, max_length=max_length)\n",
    "\n",
    "    def __init__(self, model, max_length=100):\n",
    "        self._model = model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        X = get_features([doc], self.max_length)\n",
    "        y = self._model.predict(X)\n",
    "        self.set_sentiment(doc, y)\n",
    "\n",
    "    def pipe(self, docs, batch_size=1000, n_threads=2):\n",
    "        for minibatch in cytoolz.partition_all(batch_size, docs):\n",
    "            minibatch = list(minibatch)\n",
    "            sentences = []\n",
    "            for doc in minibatch:\n",
    "                sentences.extend(doc.sents)\n",
    "            Xs = get_features(sentences, self.max_length)\n",
    "            ys = self._model.predict(Xs)\n",
    "            for sent, label in zip(sentences, ys):\n",
    "                sent.doc.sentiment += label - 0.5\n",
    "            for doc in minibatch:\n",
    "                yield doc\n",
    "\n",
    "    def set_sentiment(self, doc, y):\n",
    "        doc.sentiment = float(y[0])\n",
    "        # Sentiment has a native slot for a single float.\n",
    "        # For arbitrary data storage, there's:\n",
    "        # doc.user_data['my_data'] = y\n",
    "\n",
    "\n",
    "def get_labelled_sentences(docs, doc_labels):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for doc, y in zip(docs, doc_labels):\n",
    "        for sent in doc.sents:\n",
    "            sentences.append(sent)\n",
    "            labels.append(y)\n",
    "    return sentences, numpy.asarray(labels, dtype='int32')\n",
    "\n",
    "\n",
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            if token.has_vector and not token.is_punct and not token.is_space:\n",
    "                Xs[i, j] = token.rank + 1\n",
    "                j += 1\n",
    "                if j >= max_length:\n",
    "                    break\n",
    "    return Xs\n",
    "\n",
    "\n",
    "def train(train_texts, train_labels, dev_texts, dev_labels,\n",
    "        lstm_shape, lstm_settings, lstm_optimizer, batch_size=100, nb_epoch=5,\n",
    "        by_sentence=True):\n",
    "    print(\"Loading spaCy\")\n",
    "    nlp = spacy.load('en', entity=False)\n",
    "    embeddings = get_embeddings(nlp.vocab)\n",
    "    model = compile_lstm(embeddings, lstm_shape, lstm_settings)\n",
    "    print(\"Parsing texts...\")\n",
    "    train_docs = list(nlp.pipe(train_texts, batch_size=5000, n_threads=3))\n",
    "    dev_docs = list(nlp.pipe(dev_texts, batch_size=5000, n_threads=3))\n",
    "    if by_sentence:\n",
    "        train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)\n",
    "        dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)\n",
    "\n",
    "    train_X = get_features(train_docs, lstm_shape['max_length'])\n",
    "    dev_X = get_features(dev_docs, lstm_shape['max_length'])\n",
    "    model.fit(train_X, train_labels, validation_data=(dev_X, dev_labels),\n",
    "              nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_lstm(embeddings, shape, settings):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            embeddings.shape[0],\n",
    "            embeddings.shape[1],\n",
    "            input_length=shape['max_length'],\n",
    "            trainable=False,\n",
    "            weights=[embeddings],\n",
    "            mask_zero=True\n",
    "        )\n",
    "    )\n",
    "    model.add(TimeDistributed(Dense(shape['nr_hidden'], bias=False)))\n",
    "    model.add(Bidirectional(LSTM(shape['nr_hidden'], dropout_U=settings['dropout'],\n",
    "                                 dropout_W=settings['dropout'])))\n",
    "    model.add(Dense(shape['nr_class'], activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(lr=settings['lr']), loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_embeddings(vocab):\n",
    "    max_rank = max(lex.rank+1 for lex in vocab if lex.has_vector)\n",
    "    vectors = numpy.ndarray((max_rank+1, vocab.vectors_length), dtype='float32')\n",
    "    for lex in vocab:\n",
    "        if lex.has_vector:\n",
    "            vectors[lex.rank + 1] = lex.vector\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def evaluate(model_dir, texts, labels, max_length=100):\n",
    "    def create_pipeline(nlp):\n",
    "        '''\n",
    "        This could be a lambda, but named functions are easier to read in Python.\n",
    "        '''\n",
    "        return [nlp.tagger, nlp.parser, SentimentAnalyser.load(model_dir, nlp,\n",
    "                                                               max_length=max_length)]\n",
    "\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.pipeline = create_pipeline(nlp)\n",
    "\n",
    "    correct = 0\n",
    "    i = 0 \n",
    "    for doc in nlp.pipe(texts, batch_size=1000, n_threads=4):\n",
    "        correct += bool(doc.sentiment >= 0.5) == bool(labels[i])\n",
    "        i += 1\n",
    "    return float(correct) / i\n",
    "\n",
    "\n",
    "def read_data(data_dir, limit=0):\n",
    "    examples = []\n",
    "    for subdir, label in (('pos', 1), ('neg', 0)):\n",
    "        for filename in (data_dir / subdir).iterdir():\n",
    "            with filename.open() as file_:\n",
    "                text = file_.read()\n",
    "            examples.append((text, label))\n",
    "    random.shuffle(examples)\n",
    "    if limit >= 1:\n",
    "        examples = examples[:limit]\n",
    "    return zip(*examples) # Unzips into two lists\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    train_dir=(\"Location of training file or directory\"),\n",
    "    dev_dir=(\"Location of development file or directory\"),\n",
    "    model_dir=(\"Location of output model directory\",),\n",
    "    is_runtime=(\"Demonstrate run-time usage\", \"flag\", \"r\", bool),\n",
    "    nr_hidden=(\"Number of hidden units\", \"option\", \"H\", int),\n",
    "    max_length=(\"Maximum sentence length\", \"option\", \"L\", int),\n",
    "    dropout=(\"Dropout\", \"option\", \"d\", float),\n",
    "    learn_rate=(\"Learn rate\", \"option\", \"e\", float),\n",
    "    nb_epoch=(\"Number of training epochs\", \"option\", \"i\", int),\n",
    "    batch_size=(\"Size of minibatches for training LSTM\", \"option\", \"b\", int),\n",
    "    nr_examples=(\"Limit to N examples\", \"option\", \"n\", int)\n",
    ")\n",
    "def main(model_dir, train_dir, dev_dir,\n",
    "         is_runtime=False,\n",
    "         nr_hidden=64, max_length=100, # Shape\n",
    "         dropout=0.5, learn_rate=0.001, # General NN config\n",
    "         nb_epoch=5, batch_size=100, nr_examples=-1):  # Training params\n",
    "    model_dir = pathlib.Path(model_dir)\n",
    "    train_dir = pathlib.Path(train_dir)\n",
    "    dev_dir = pathlib.Path(dev_dir)\n",
    "    if is_runtime:\n",
    "        dev_texts, dev_labels = read_data(dev_dir)\n",
    "        acc = evaluate(model_dir, dev_texts, dev_labels, max_length=max_length)\n",
    "        print(acc)\n",
    "    else:\n",
    "        print(\"Read data\")\n",
    "        train_texts, train_labels = read_data(train_dir, limit=nr_examples)\n",
    "        dev_texts, dev_labels = read_data(dev_dir, limit=nr_examples)\n",
    "        train_labels = numpy.asarray(train_labels, dtype='int32')\n",
    "        dev_labels = numpy.asarray(dev_labels, dtype='int32')\n",
    "        lstm = train(train_texts, train_labels, dev_texts, dev_labels,\n",
    "                     {'nr_hidden': nr_hidden, 'max_length': max_length, 'nr_class': 1},\n",
    "                     {'dropout': dropout, 'lr': learn_rate},\n",
    "                     {},\n",
    "                     nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "        weights = lstm.get_weights()\n",
    "        with (model_dir / 'model').open('wb') as file_:\n",
    "            pickle.dump(weights[1:], file_)\n",
    "        with (model_dir / 'config.json').open('wb') as file_:\n",
    "            file_.write(lstm.to_json())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plac.call(main) # calls main function \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%python3 05_Bidirectional_LSTM_classifier.py model train test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
